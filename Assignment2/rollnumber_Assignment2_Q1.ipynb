{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 2 - Question 1\n",
    "The objective of this assignment is to get you familiarize with  the  problem  of  `Clustering`.\n",
    "\n",
    "## Instructions\n",
    "- Write your code and analysis in the indicated cells.\n",
    "- Ensure that this notebook runs without errors when the cells are run in sequence.\n",
    "- Do not attempt to change the contents of other cells.\n",
    "- No inbuilt functions to be used until specified\n",
    "\n",
    "## Submission\n",
    "- Ensure that this notebook runs without errors when the cells are run in sequence.\n",
    "- Rename the notebook to `<roll_number>_Assignment2_Q1.ipynb`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TdEXGGO0AqkW",
    "outputId": "119fb7e5-3656-4bdb-fcfa-2e33afe2d0d3"
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "# nltk.download('punkt')    \n",
    "# if u r facing issues while importing nltk, please uncomment above line and run\n",
    "import re\n",
    "import numpy as np\n",
    "import sklearn\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "id": "5N2FkdFMxixH"
   },
   "outputs": [],
   "source": [
    "# !pip install sentence-transformers\n",
    "from sentence_transformers import SentenceTransformer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s-1irtUhxlp2"
   },
   "source": [
    "# Dataset\n",
    "\n",
    "\n",
    "\n",
    "*   Try to explore the dataset and first understand\n",
    "*   Steps while processing the dataset:\n",
    "\n",
    "1.   Load the dataset\n",
    ">> The 20 newsgroups dataset comprises around 18000 newsgroups posts on 20 topics split in two subsets: train and test. Here, we only use train part of the dataset as we don't need any training.\n",
    "\n",
    "2.   pre-processing of the dataset\n",
    ">>   A set of basic pre-processing steps are given below, if you can do it better, it is appreciable\n",
    "3.   Trying to obtain the embeddings for the text. \n",
    ">> Here, we used bert model to obtain the embeddings, if you want to use anyother sentence/word embeddings (ELMo,universal sentence encoder, or other bert models) you can use it, but not mandatorily change it)\n",
    "\n",
    "PS: You need not completely understand how bert works. If you are interested, few links will be mentioned below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "id": "oqkn6iSNmEo1"
   },
   "outputs": [],
   "source": [
    "# loading of dataset\n",
    "\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "newsgroups_train = fetch_20newsgroups(subset='train')\n",
    "\n",
    "# print(list(newsgroups_train))\n",
    "#['data', 'filenames', 'target_names', 'target', 'DESCR']\n",
    "# all we require for our task is data and target. \n",
    "#target_names describe the different groups present (which are 20) all over the dataset\n",
    "\n",
    "# print(list(newsgroups_train.target_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "id": "NNRbIfsX01hD"
   },
   "outputs": [],
   "source": [
    "bert_model = SentenceTransformer('bert-base-nli-mean-tokens')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "id": "kp8UL8lP5U2k"
   },
   "outputs": [],
   "source": [
    "#preprocessing of sentences and the article\n",
    "\n",
    "def remove_punct(text):\n",
    "  text = re.sub('[^a-zA-Z0-9 ]+','', text)\n",
    "  return text\n",
    "\n",
    "def remove_urls(text):\n",
    "  url_pattern = re.compile(r'https?://\\S+|www\\.\\S+')\n",
    "  return url_pattern.sub(r'', text)\n",
    "\n",
    "def remove_tag(text):   \n",
    "  text=' '.join(text)\n",
    "  html_pattern = re.compile('<.*?>')\n",
    "  return html_pattern.sub(r'', text)\n",
    "\n",
    "def pre_process_sentence(sentence):\n",
    "  sentence = sentence.lower()\n",
    "  sentence = remove_punct(remove_urls(sentence))\n",
    "  return sentence\n",
    "\n",
    "def pre_process_article(article):\n",
    "  article = str(article).replace(\"\\n\", '')\n",
    "  article = sent_tokenize(article)\n",
    "  sentences = []\n",
    "  for each in article:\n",
    "    if len(each.split(\":\")) > 1:\n",
    "      continue\n",
    "    sentences.append(pre_process_sentence(each))\n",
    "  return sentences\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "id": "72IcGYs980zK"
   },
   "outputs": [],
   "source": [
    "\n",
    "def get_review_embedding(article):\n",
    "  sentences = pre_process_article(article)\n",
    "\n",
    "  #here review(input) has to be a list of sentences\n",
    "  #use suitable embeddings to get an embedding for the whole review\n",
    "  #usage of sentence embeddings is recommended\n",
    "\n",
    "  sentence_embeddings = bert_model.encode(sentences)\n",
    "\n",
    "  # take average of all sentence embeddings to obtain a review embedding \n",
    "  review_embedding = np.zeros(768)\n",
    "  for each in sentence_embeddings:\n",
    "    review_embedding = np.add(np.array(each), review_embedding)\n",
    "\n",
    "  return review_embedding, sentence_embeddings, sentences\n",
    "\n",
    "def get_all_embeddings(articles):\n",
    "  avg_embeddings = []\n",
    "  all_embeddings = []\n",
    "  preprocessed_sentences = []\n",
    "  for i in range(0, len(articles)):\n",
    "    temp1, temp2, temp3 = get_review_embedding(articles[i])\n",
    "    avg_embeddings.append(temp1)\n",
    "    all_embeddings.append(temp2)\n",
    "    preprocessed_sentences.append(temp3)\n",
    "  return avg_embeddings, all_embeddings, preprocessed_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pbkl4Ux0JPi2"
   },
   "outputs": [],
   "source": [
    "avg_embeddings, all_embeddings, preprocessed_sentences = get_all_embeddings(newsgroups_train.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JkaKG4WW9eXx"
   },
   "outputs": [],
   "source": [
    "# data visualization \n",
    "\n",
    "# Try to visualise the points from all the domains and try to visualise them \n",
    "# hint: you can use PCA \n",
    "\n",
    "import sklearn\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uZH6gO7HA5jk"
   },
   "source": [
    "# K_Means Algorithm\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "T9TBSHWfgWxU",
    "outputId": "8d4ab613-7513-4732-bf06-8b7e7cd8a2c9"
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (182033706.py, line 22)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Input \u001b[0;32mIn [37]\u001b[0;36m\u001b[0m\n\u001b[0;31m    def\u001b[0m\n\u001b[0m        ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "# code to write your Kmeans algorithm\n",
    "#implement your KMeans algorithm here, and visualise the clusters obtained \n",
    "\n",
    "# from sklearn.cluster import KMeans\n",
    "\n",
    "# kmeans = KMeans(n_clusters=2, random_state=4).fit(X)\n",
    "# kmeans.labels_\n",
    "\n",
    "# my implementation of the KMeans algorithm where I used \n",
    "# word embeddings for preparing a 768 dimensional representation\n",
    "class MyKMeans():\n",
    "    # option to use KMeans++ while calling this function\n",
    "\n",
    "    def __init__(self, n_cluster, avg_embeddings, algo_type):\n",
    "        self.n_cluster = n_cluster\n",
    "        self.avg_embeddings = avg_embeddings\n",
    "        # self.all_embeddings = all_embeddings\n",
    "        self.data_size = len(avg_embeddings)\n",
    "        self.dims = len(avg_embeddings[0])\n",
    "        self.optimization = False\n",
    "        if algo_type == '++':\n",
    "            self.optimization = True\n",
    "\n",
    "    # select points (random if not KMeans++)\\\n",
    "    def __initialPointsRand(self):\n",
    "        x = np.rand(np.arange(self.data_size), self.n_cluster)\n",
    "        return x\n",
    "    \n",
    "    def __findDistance(self, pt1, pt2):\n",
    "        dist = 0\n",
    "        for i in range(len(self.avg_embeddings[0])):\n",
    "            dist += (self.avg_embeddings[pt1][i] - self.avg_embeddings[pt2][i])**2\n",
    "        return dist**(0.5)\n",
    "\n",
    "    # start fitting the points (findout if there are optimal ways to find these distances)\n",
    "    def fit(self, rounds):\n",
    "\n",
    "        def __hasChanged(self, prev_clusters):\n",
    "            for i in range(len(self.centers)):\n",
    "                point  = self.centers[i]\n",
    "                for cluster in prev_clusters:\n",
    "                    if point in cluster:\n",
    "                        if self.clusters[i] != cluster:\n",
    "                            return True\n",
    "            return False\n",
    "        \n",
    "        self.centers = self.__initialPointsRand()\n",
    "        self.clusters = [set(i) for i in range(self.n_cluster)]\n",
    "        prev_clusters = [set() for i in range(self.n_cluster)]\n",
    "        while __hasChanged(prev_clusters):\n",
    "            prev_clusters = self.clusters\n",
    "            for pt in range(self.n_cluster):\n",
    "                min_dist = 1e150\n",
    "                best_cluster = -1\n",
    "                for i in range(self.dims):\n",
    "                    dist = self.__findDistance(pt, self.centers[i])\n",
    "                    if min_dist > dist:\n",
    "                        min_dist = dist\n",
    "                        best_cluster = i\n",
    "\n",
    "                assert(best_cluster > -1 and best_cluster < self.dims)\n",
    "                self.clusters[best_cluster].add(pt)\n",
    "\n",
    "    # can do above for some number of times so that we get better results\n",
    "\n",
    "    # won't return anything, just store in the object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "61GQRof2wkKd"
   },
   "outputs": [],
   "source": [
    "#code for visualisation of clusters\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m2t9Cz6yuJmO"
   },
   "source": [
    "# Elbow method\n",
    "\n",
    "\n",
    "\n",
    "*   Try to understand how elbow method works\n",
    "*   Plot the graph between average distance and the number of clusters\n",
    "*   Use elbow method to find the optimal number of clusters, \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JW_stdJuub4s"
   },
   "outputs": [],
   "source": [
    "def elbow_method():\n",
    "\n",
    "\n",
    "  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2jHeGh-xuuDc"
   },
   "source": [
    "# Silhouette Method\n",
    "\n",
    "\n",
    "*   Compute silhouette score varying the K number of clusters\n",
    "\n",
    "*   Plot the graph between silhoutte score and number of clusters \n",
    "\n",
    "*   Find the optimal number of clusters using silhouette method\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "> Report the optimal number of clusters you obtained from above two methods (elbow and silhouette)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lKRJ4IBMuwfd"
   },
   "outputs": [],
   "source": [
    "def silhouette_score():\n",
    "\n",
    "  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JEvLJxmnuCof"
   },
   "source": [
    "# Agglomerative clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nzssThP1g_NN",
    "outputId": "27d005d2-7405-4a81-a644-f35bdd3ef4f1"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# code to write your Kmeans algorithm\n",
    "#implement your KMeans algorithm here, and visualise the clusters obtained \n",
    "\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "\n",
    "def AgglomerativeClustering():\n",
    "  clustering = AgglomerativeClustering().fit(visuals)\n",
    "  clustering.labels_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "whf5OLe2wf6o"
   },
   "outputs": [],
   "source": [
    "#code for visualisation of clusters\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZdNC7XxxwuJW"
   },
   "source": [
    "# Dendogram\n",
    "\n",
    "\n",
    "*   Try to understand the difference between agglomerative clustering and hierarchical clustering\n",
    "*   Plot dendograms for both kinds of clustering\n",
    "*   Find the optimal number of clusters with the help of Dendogram\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1NWavQn0hW3i"
   },
   "outputs": [],
   "source": [
    "# code to write dendogram\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B3On7lpA11eB"
   },
   "source": [
    "# useful links to understand BERT\n",
    "\n",
    "*  https://towardsdatascience.com/bert-explained-state-of-the-art-language-model-for-nlp-f8b21a9b6270#:~:text=How%20BERT%20works,%2Dwords)%20in%20a%20text.&text=As%20opposed%20to%20directional%20models,sequence%20of%20words%20at%20once.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "Q_Clustering.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
