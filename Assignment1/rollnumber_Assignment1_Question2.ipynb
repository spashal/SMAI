{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "866ed254",
      "metadata": {
        "id": "866ed254"
      },
      "source": [
        "# Approximate Nearest Neighbors:\n",
        "\n",
        "# Image Recommendation System via Collaborative Filtering"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X1q_aLJJW4QB",
        "outputId": "b9263d7a-be1e-40e8-9996-398f23e45a3a"
      },
      "id": "X1q_aLJJW4QB",
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2e08937a",
      "metadata": {
        "id": "2e08937a"
      },
      "source": [
        "# ***Please read the instructions very carefully***\n",
        "This is a modified version of the previous question and requires you to use an artificial nearest neighbors library\n",
        "\n",
        "We suggest you to use one of the following:\n",
        "- [ScaNN](https://github.com/google-research/google-research/tree/master/scann)\n",
        "- [FAISS](https://github.com/facebookresearch/faiss)\n",
        "- [Annoy](https://github.com/spotify/annoy.git)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "489a4212",
      "metadata": {
        "id": "489a4212"
      },
      "source": [
        "1.   Assignment must be implemented in Python 3 only.\n",
        "2.   You are allowed to use libraries for data preprocessing (numpy, pandas, nltk etc) and for evaluation metrics, data visualization (matplotlib etc.).\n",
        "3.   You will be evaluated not just on the overall performance of the model and also on the experimentation with hyper parameters, data prepossessing techniques etc.\n",
        "4.   ⚠️ The Assignment will be evaluated automatically. Please adhere to taking proper inputs from `config.csv` file. You can change your `config.csv` file to experiment with your code. But at the end, make sure that your outputs are corresponding to input values in `config.csv`\n",
        "5.   Strict plagiarism checking will be done. An F will be awarded for plagiarism."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f494560c",
      "metadata": {
        "id": "f494560c"
      },
      "source": [
        "## About the Dataset\n",
        "Behance is a community art website where users showcase and discover creative work. Each user is able to “appreciate” (equivalent to a “like” on Instagram or a “react” on Facebook) an image, indicating that they like the image. It is in the website’s best interests to show users pictures that they would like, to keep them engaged for longer. For this question, given a set of pictures that a user has already appreciated, you have to show them a new picture that they would like based on what similar users appreciated.\n",
        "\n",
        "\n",
        "<br><br>\n",
        "**The dataset has information of 1 million appreciates of 63,497 users on 178,788 items. The file Behance appreciate 1M has a triplet in each line in the form of (user id, item id, unix timestamp).**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d8dd3f15",
      "metadata": {
        "id": "d8dd3f15"
      },
      "source": [
        "**Task: Take the inputs from the config.csv file and output the recommendations for a particular person**\n",
        "- Collaborative Filtering is a way to predict items to the user based on the the\n",
        "user’s history and the history of similar users. The similarity between users can be quantified by the number of images that both the users appreciated.\n",
        "- The images appreciated by a similar user would be the most suitable images to show a user. Since we can find the similarity between any two users, we would be able to find the “nearest” neighbours of any user, allowing us to use a KNN-based algorithm to recommend new images to a user.\n",
        "- Since people do not like seeing pictures that they have seen already. Make sure that you do not recommend pictures that a user has appreciated already.\n",
        "- Output the final response will be saved in the file named ```config['output_file']```.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "68a52fe5",
      "metadata": {
        "id": "68a52fe5"
      },
      "source": [
        "**Output file format:**\n",
        "Populate the output file with images that the user has not seen of the k most\n",
        "similar users, in descending order of their similarity. Each line in the output\n",
        "file should be a duplet in the form of (item id, user id), where the user id is the\n",
        "id of the kth similar user. The order of the images corresponding to the same\n",
        "similar user would not matter. The output file would look something like this:\n",
        "```\n",
        "item_id_1_of_1st_similar_user 1st_most_similar_user_id\n",
        "item_id_2_of_1st_similar_user 1st_most_similar_user_id\n",
        "item_id_3_of_1st_similar_user 1st_most_similar_user_id\n",
        "...\n",
        "item_id_1_of_2nd_similar_user 2nd_most_similar_user_id\n",
        "item_id_2_of_2nd_similar_user 2nd_most_similar_user_id\n",
        "item_id_3_of_2nd_similar_user 2nd_most_similar_user_id\n",
        "...\n",
        "item_id_1_of_kth_similar_user kth_most_similar_user_id\n",
        "item_id_2_of_kth_similar_user kth_most_similar_user_id\n",
        "item_id_3_of_kth_similar_user kth_most_similar_user_id\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "26d46fd2",
      "metadata": {
        "id": "26d46fd2"
      },
      "source": [
        "You may use any other recommendation system that you wish to use. However,\n",
        "evaluation script will score your submission by measuring the similarity between\n",
        "users with the number of common images they appreciated.\n",
        "The dataset was extracted using Behance’s API as a part of the paper\n",
        "“Vista: A visually, socially, and temporally-aware model for artistic\n",
        "recommendation, RecSys, 2016”. Check out this [Google Drive folder](https://drive.google.com/drive/folders/0B9Ck8jw-TZUEc3NlMjVXdDlPU1k?resourcekey=0-6_8ykn0o4fLc5fuTEm91xA) for\n",
        "more information about the dataset.\n",
        "\n",
        "\n",
        "Have fun! The users are waiting to see new pictures!"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "912088cf",
      "metadata": {
        "id": "912088cf"
      },
      "source": [
        "### Import necessary libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "B0tdNFxtRJZ8",
      "metadata": {
        "id": "B0tdNFxtRJZ8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "059737ef-a003-4b18-9c9e-5b6f41299e15"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.7/dist-packages (2.7.0)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (0.23.1)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: libclang>=9.0.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (12.0.0)\n",
            "Requirement already satisfied: absl-py>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.0.0)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (3.3.0)\n",
            "Requirement already satisfied: gast<0.5.0,>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (0.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: numpy>=1.14.5 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.19.5)\n",
            "Requirement already satisfied: keras<2.8,>=2.7.0rc0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (2.7.0)\n",
            "Requirement already satisfied: keras-preprocessing>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.1.2)\n",
            "Requirement already satisfied: protobuf>=3.9.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (3.17.3)\n",
            "Requirement already satisfied: tensorflow-estimator<2.8,~=2.7.0rc0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (2.7.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.32.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (0.37.1)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (3.1.0)\n",
            "Requirement already satisfied: tensorboard~=2.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (2.7.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.1.0)\n",
            "Requirement already satisfied: flatbuffers<3.0,>=1.12 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (2.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.15.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.43.0)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.13.3)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (3.10.0.2)\n",
            "Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py>=2.9.0->tensorflow) (1.5.2)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow) (1.35.0)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow) (1.8.1)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow) (1.0.1)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow) (2.23.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow) (0.6.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow) (3.3.6)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow) (57.4.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow) (0.4.6)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard~=2.6->tensorflow) (4.2.4)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard~=2.6->tensorflow) (0.2.8)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard~=2.6->tensorflow) (4.8)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.6->tensorflow) (1.3.0)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard~=2.6->tensorflow) (4.10.1)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard~=2.6->tensorflow) (3.7.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard~=2.6->tensorflow) (0.4.8)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow) (2021.10.8)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow) (2.10)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.6->tensorflow) (3.1.1)\n",
            "Collecting scann\n",
            "  Downloading scann-1.2.4-cp37-cp37m-manylinux2014_x86_64.whl (10.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 10.6 MB 4.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from scann) (1.19.5)\n",
            "Requirement already satisfied: tensorflow~=2.7.0 in /usr/local/lib/python3.7/dist-packages (from scann) (2.7.0)\n",
            "Requirement already satisfied: keras-preprocessing>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow~=2.7.0->scann) (1.1.2)\n",
            "Requirement already satisfied: protobuf>=3.9.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow~=2.7.0->scann) (3.17.3)\n",
            "Requirement already satisfied: gast<0.5.0,>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow~=2.7.0->scann) (0.4.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow~=2.7.0->scann) (1.43.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow~=2.7.0->scann) (3.1.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.32.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow~=2.7.0->scann) (0.37.1)\n",
            "Requirement already satisfied: keras<2.8,>=2.7.0rc0 in /usr/local/lib/python3.7/dist-packages (from tensorflow~=2.7.0->scann) (2.7.0)\n",
            "Requirement already satisfied: tensorboard~=2.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow~=2.7.0->scann) (2.7.0)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow~=2.7.0->scann) (1.13.3)\n",
            "Requirement already satisfied: tensorflow-estimator<2.8,~=2.7.0rc0 in /usr/local/lib/python3.7/dist-packages (from tensorflow~=2.7.0->scann) (2.7.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow~=2.7.0->scann) (1.6.3)\n",
            "Requirement already satisfied: absl-py>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow~=2.7.0->scann) (1.0.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow~=2.7.0->scann) (1.15.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow~=2.7.0->scann) (1.1.0)\n",
            "Requirement already satisfied: flatbuffers<3.0,>=1.12 in /usr/local/lib/python3.7/dist-packages (from tensorflow~=2.7.0->scann) (2.0)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow~=2.7.0->scann) (3.3.0)\n",
            "Requirement already satisfied: libclang>=9.0.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow~=2.7.0->scann) (12.0.0)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow~=2.7.0->scann) (0.23.1)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow~=2.7.0->scann) (0.2.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow~=2.7.0->scann) (3.10.0.2)\n",
            "Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py>=2.9.0->tensorflow~=2.7.0->scann) (1.5.2)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow~=2.7.0->scann) (57.4.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow~=2.7.0->scann) (1.0.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow~=2.7.0->scann) (3.3.6)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow~=2.7.0->scann) (0.4.6)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow~=2.7.0->scann) (1.35.0)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow~=2.7.0->scann) (1.8.1)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow~=2.7.0->scann) (2.23.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow~=2.7.0->scann) (0.6.1)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard~=2.6->tensorflow~=2.7.0->scann) (4.8)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard~=2.6->tensorflow~=2.7.0->scann) (0.2.8)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard~=2.6->tensorflow~=2.7.0->scann) (4.2.4)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.6->tensorflow~=2.7.0->scann) (1.3.0)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard~=2.6->tensorflow~=2.7.0->scann) (4.10.1)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard~=2.6->tensorflow~=2.7.0->scann) (3.7.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard~=2.6->tensorflow~=2.7.0->scann) (0.4.8)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow~=2.7.0->scann) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow~=2.7.0->scann) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow~=2.7.0->scann) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow~=2.7.0->scann) (2021.10.8)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.6->tensorflow~=2.7.0->scann) (3.1.1)\n",
            "Installing collected packages: scann\n",
            "Successfully installed scann-1.2.4\n"
          ]
        }
      ],
      "source": [
        "%pip install tensorflow\n",
        "%pip install scann"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "13e9cbe3",
      "metadata": {
        "id": "13e9cbe3"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os, time, sys, scann\n",
        "from scipy.sparse.linalg import svds\n",
        "from scipy.sparse import csr_matrix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "cb6bc1ff",
      "metadata": {
        "id": "cb6bc1ff"
      },
      "outputs": [],
      "source": [
        "config = pd.read_csv('config.csv').iloc[0]\n",
        "# config = {'id':276633,\n",
        "#           'k':5,\n",
        "#           'dataset_file':'./Behance_appreciate_1M',\n",
        "#           'output_file':'./output.txt'}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "ef721d6b",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ef721d6b",
        "outputId": "cd28354e-d13c-4d75-8516-8bca9c2727a4"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'dataset_file': './Behance_appreciate_1M',\n",
              " 'id': 276633,\n",
              " 'k': 5,\n",
              " 'output_file': './output.txt'}"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "config"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "4966a889",
      "metadata": {
        "id": "4966a889"
      },
      "outputs": [],
      "source": [
        "user = config['id']\n",
        "k_value = config['k']"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e8e50dee",
      "metadata": {
        "id": "e8e50dee"
      },
      "source": [
        "### Read the Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "cd74173b",
      "metadata": {
        "id": "cd74173b"
      },
      "outputs": [],
      "source": [
        "with open(config['dataset_file'], 'r') as inFile:\n",
        "    appreciate_data = inFile.readlines()\n",
        "# with open('/content/drive/MyDrive/Behance_appreciate_1M', 'r') as inFile:\n",
        "#     appreciate_data = inFile.readlines()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "e8542cda",
      "metadata": {
        "id": "e8542cda"
      },
      "outputs": [],
      "source": [
        "# your code here"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e09e9961",
      "metadata": {
        "id": "e09e9961"
      },
      "source": [
        "### Initialize a dictionary to store the item_ids that a user likes\n",
        "\n",
        "### Go through each line of the input file and construct the user_likes dictionary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "8ff4a80d",
      "metadata": {
        "id": "8ff4a80d"
      },
      "outputs": [],
      "source": [
        "user_likes = dict()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "5b55dc0b",
      "metadata": {
        "id": "5b55dc0b"
      },
      "outputs": [],
      "source": [
        "userData = dict()                   # userData is the final preprocessed data\n",
        "users_dict = dict()\n",
        "images_dict = dict()\n",
        "processed_data = np.empty((63497, 178788), dtype=bool)\n",
        "\n",
        "for line in appreciate_data:\n",
        "  val = line.split(' ')\n",
        "  user = int(val[0])\n",
        "  image = int(val[1])\n",
        "\n",
        "  if user not in users_dict:\n",
        "    users_dict[user] = len(users_dict)\n",
        "    userData[users_dict[user]] = []\n",
        "  user_id = users_dict[user]\n",
        "  if image not in images_dict:\n",
        "    images_dict[image] = len(images_dict)\n",
        "  image_id = images_dict[image]\n",
        "\n",
        "  userData[user_id].append(image)\n",
        "  processed_data[user_id][image_id] = True\n",
        "\n",
        "\n",
        "denseMatrix = csr_matrix(processed_data, dtype=float)\n",
        "processed_data = 0\n",
        "reducedMatrix, s, vt = svds(denseMatrix, k = 1000)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "fu1Qw8myRn9w",
      "metadata": {
        "id": "fu1Qw8myRn9w"
      },
      "outputs": [],
      "source": [
        "def compute_recall(neighbors, true_neighbors):\n",
        "    total = 0\n",
        "    for gt_row, row in zip(true_neighbors, neighbors):\n",
        "        total += np.intersect1d(gt_row, row).shape[0]\n",
        "    return total / true_neighbors.size"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cd017880",
      "metadata": {
        "id": "cd017880"
      },
      "source": [
        "### Use your choice of Approximate Nearest Neigbor after Collaborative Filtering to find nearest neighbors"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "5ee1d377",
      "metadata": {
        "id": "5ee1d377"
      },
      "outputs": [],
      "source": [
        "# your code here\n",
        "normalized_dataset = reducedMatrix / np.linalg.norm(reducedMatrix, axis=1)[:, np.newaxis]\n",
        "\n",
        "searcher = scann.scann_ops_pybind.builder(normalized_dataset, 10, \"dot_product\").tree(\n",
        "    num_leaves=2000, num_leaves_to_search=100, training_sample_size=reducedMatrix.shape[0]).score_ah(\n",
        "    2, anisotropic_quantization_threshold=0.2).reorder(100).build()\n",
        "def neighbors(user,k_value):\n",
        "  global normalized_dataset, searcher\n",
        "  \n",
        "  queries = [user]\n",
        "  neighbors, distances = searcher.search(queries, leaves_to_search=150)\n",
        "  return neighbors[0,:k_value]\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "GeGFUS5qZsUE",
      "metadata": {
        "id": "GeGFUS5qZsUE"
      },
      "outputs": [],
      "source": [
        "def neighbors(user,k_value):\n",
        "  global normalized_dataset, searcher, users_dict\n",
        "  \n",
        "  neighbors, distances = searcher.search(normalized_dataset[users_dict[user]], leaves_to_search=150)\n",
        "  return neighbors[:k_value]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "13819VBbT-Ao",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "13819VBbT-Ao",
        "outputId": "a5bfbb1d-88e0-4779-abf7-4e166dc102a5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[63496 33929 38741 41294 42540]\n"
          ]
        }
      ],
      "source": [
        "print(neighbors(user, k_value))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0b6f24d6",
      "metadata": {
        "id": "0b6f24d6"
      },
      "source": [
        "### Answer the following questions:"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "012f72f8",
      "metadata": {
        "id": "012f72f8"
      },
      "source": [
        "#### Q1. **Explain how your choice of library works**\n",
        "The library that I have employed in this question is Google's **Scalable Nearest Neighbors** (ScaNN). It is one of the most recent advancements in KNN algorithm and performs the best for large number of queries (the usecase that google needed it for).\n",
        "\n",
        "It uses *maximum inner-product* (MIPS) as the distance metric. It uses a new solution to compress the embeddings which is to quantize the vectors into one of certain allowed representations thereby decreasing the search space. The quantization is performed by *anisotropic vector quantization* where in a vector is quantized to another vector such that it has the least loss in the direction of the data-point x, not the one which is closest to it. This way, the MIPS is optimized to decrease the time per query with the minimal loss of information.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3b0ac14b",
      "metadata": {
        "id": "3b0ac14b"
      },
      "source": [
        "#### Q2. **Compare your choice of library with vanilla KNN.**\n",
        "***Hint: Include Time Complexity, and explain the tradeoff with recall***\n",
        "\n",
        "\n",
        "### Ans\n",
        "\n",
        "\n",
        "A normal KNN algorithm (aka vanilla KNN) computes distance between all pair of points. This ensures the best abidance to KNN algorithm but is very very slow for practical purposes. This solution offers best recall but the tradeoff with speed is very poor.\n",
        "\n",
        "\n",
        "On the other hand, ScaNN has a good speed-recall tradeoff since it uses approximations to lower the distance calculation cost. It uses vector quantization with minimal loss of information. Due to this same reason, it is fairly accurate and scalable as well"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "400d7827",
      "metadata": {
        "id": "400d7827"
      },
      "source": [
        "#### Q3. **Compare your choice of library with implementation of ScaNN, faiss and annoy.**\n",
        "***Hint: Include Time Complexity, and explain the tradeoff with recall***\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b2dc0f67",
      "metadata": {
        "id": "b2dc0f67"
      },
      "source": [
        "\n",
        "#### Ans\n",
        "\n",
        "\n",
        "The chosen implementation is ScaNN. It uses anisotropic vector quantization to achieve better vector approximations. But this method does not save it any time. The reason why ScaNN is the fastest and most accurate KNN algorithm for public use is that it uses in-register lookups for saving time. This also allows it to have a better trade-off with recall. The following graphs shows it.\n",
        "\n",
        "\n",
        "![alt text for screen readers](tradeoff.png)\n",
        "\n",
        "\n",
        "Comparing ScaNN with Annoy (by Spotify), ScaNN is not exceptionally fast because it has a very large set of indexes (or embeddings) hence it chooses to optimize on solving the problem of the size of index files. \n",
        "\n",
        "It has a system of having a tree structure for indexes and allowing multiple processes to use the same set of indexes allowing parallel computing. Visibly, the speed tradeoff with recall is not so much as good as ScaNN, but that is only due to a difference in the usecase (Sptofity, Uber, Instacart don't use it for nothing!)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "142440b0",
      "metadata": {
        "id": "142440b0"
      },
      "source": [
        "### Open the output file to write all the lines to the file"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "id": "977b08f9",
      "metadata": {
        "id": "977b08f9"
      },
      "outputs": [],
      "source": [
        "outFile = open(config['output_file'], 'w')\n",
        "\n",
        "for n_user in neighbors(user,k_value):\n",
        "    user_id = n_user\n",
        "    real_id = 0\n",
        "    for item in users_dict:\n",
        "      if users_dict[item] == user_id:\n",
        "        real_id = item\n",
        "        break\n",
        "    for item_id in userData[user_id]:\n",
        "        if item_id not in userData[users_dict[user]]:\n",
        "            outFile.write(str(item_id) + ' ' + str(real_id) + '\\n')\n",
        "\n",
        "outFile.close()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "rollnumber_Assignment1_Question2.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}